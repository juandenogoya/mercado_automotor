# Fase 1: Preparaci√≥n de Datos Transaccionales

## üìã Objetivo

Unificar los datasets de **Inscripciones**, **Transferencias** y **Prendas** en un solo dataset optimizado para forecasting, con features temporales y agregadas.

---

## üéØ ¬øQu√© hace esta fase?

1. ‚úÖ **Explora** la estructura de las 3 tablas en PostgreSQL
2. ‚úÖ **Verifica** compatibilidad de columnas
3. ‚úÖ **Une** los 3 datasets en uno solo
4. ‚úÖ **Agrega** columna `tipo_operacion` para identificar origen
5. ‚úÖ **Crea** features temporales (a√±o, mes, trimestre, d√≠a_semana, etc.)
6. ‚úÖ **Crea** features agregadas (categor√≠as, IDs, etc.)
7. ‚úÖ **Guarda** en formato Parquet optimizado
8. ‚úÖ **Valida** calidad de datos

---

## üìÅ Archivos Creados

```
backend/data_processing/
‚îú‚îÄ‚îÄ 01_explorar_estructura_tablas.py    # Explora estructura de tablas PostgreSQL
‚îú‚îÄ‚îÄ 02_unir_datasets.py                 # Une datasets y crea features
‚îú‚îÄ‚îÄ 03_analisis_exploratorio.py         # An√°lisis estad√≠stico del dataset unificado
‚îî‚îÄ‚îÄ README_FASE1.md                     # Esta documentaci√≥n
```

---

## üöÄ Instrucciones de Uso

### **Paso 1: Verificar Requisitos**

Aseg√∫rate de tener:
- PostgreSQL corriendo en `localhost:5432`
- Base de datos `mercado_automotor` con datos cargados
- Python 3.8+ con las siguientes librer√≠as:
  ```bash
  pip install sqlalchemy pandas numpy pyarrow psycopg2-binary
  ```

### **Paso 2: Explorar Estructura de Tablas (Opcional)**

Este script te muestra las columnas de cada tabla y verifica compatibilidad:

```bash
# Desde el directorio ra√≠z: mercado_automotor/
python backend/data_processing/01_explorar_estructura_tablas.py
```

**Salida esperada:**
- Lista de columnas por tabla
- Tipos de datos
- Total de registros
- Columnas comunes entre las 3 tablas

### **Paso 3: Unir Datasets y Crear Features** ‚≠ê

Este es el script principal que genera el dataset unificado:

```bash
# Desde el directorio ra√≠z: mercado_automotor/
python backend/data_processing/02_unir_datasets.py
```

**Lo que hace:**
1. Extrae datos de las 3 tablas: `datos_gob_inscripciones`, `datos_gob_transferencias`, `datos_gob_prendas`
2. Agrega columna `tipo_operacion` con valores: `'inscripcion'`, `'transferencia'`, `'prenda'`
3. Une los 3 datasets con `pd.concat()`
4. Ordena por `tramite_fecha`
5. Crea **features temporales**:
   - `anio`, `mes`, `dia`, `trimestre`
   - `dia_semana`, `dia_semana_nombre`
   - `semana_anio`, `mes_nombre`
   - `es_fin_semana`, `es_inicio_mes`, `es_fin_mes`
   - `mes_sin`, `mes_cos` (features c√≠clicas para estacionalidad)
   - `dia_semana_sin`, `dia_semana_cos`
   - `dias_desde_origen`
6. Crea **features agregadas**:
   - `operacion_id` (ID secuencial √∫nico)
   - `marca_categoria` (Top 10 marcas + "OTROS")
   - `tipo_categoria` (Top 10 tipos + "OTROS")
7. Guarda en **Parquet** (formato optimizado, compresi√≥n snappy)

**Archivos generados:**
```
data/processed/
‚îú‚îÄ‚îÄ dataset_transaccional_unificado.parquet       # Dataset completo (Parquet)
‚îî‚îÄ‚îÄ dataset_transaccional_unificado_sample.csv    # Muestra (primeros 1000 registros)
```

### **Paso 4: An√°lisis Exploratorio (Opcional)**

Analiza el dataset unificado generado:

```bash
# Desde el directorio ra√≠z: mercado_automotor/
python backend/data_processing/03_analisis_exploratorio.py
```

**Lo que hace:**
- Estad√≠sticas descriptivas
- An√°lisis temporal (mensual, anual, estacionalidad)
- An√°lisis por provincia
- An√°lisis de veh√≠culos (marcas, tipos)
- Detecci√≥n de outliers
- Resumen ejecutivo

---

## üìä Dataset Unificado - Estructura

### **Columnas Principales**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `tramite_fecha` | datetime | Fecha de la operaci√≥n |
| `tipo_operacion` | string | 'inscripcion', 'transferencia', 'prenda' |
| `registro_seccional_provincia` | string | Provincia del registro |
| `registro_seccional_descripcion` | string | Descripci√≥n del registro seccional |
| `automotor_origen` | string | Nacional/Importado |
| `automotor_marca_descripcion` | string | Marca del veh√≠culo |
| `automotor_tipo_descripcion` | string | Tipo de veh√≠culo (sedan, pick-up, etc.) |
| `automotor_modelo_descripcion` | string | Modelo del veh√≠culo |
| `automotor_uso` | string | Particular/Comercial |
| `automotor_anio_modelo` | int | A√±o modelo del veh√≠culo |

### **Features Temporales**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `anio` | int | A√±o de la operaci√≥n |
| `mes` | int | Mes (1-12) |
| `dia` | int | D√≠a del mes (1-31) |
| `trimestre` | int | Trimestre (1-4) |
| `dia_semana` | int | D√≠a de la semana (0=Lunes, 6=Domingo) |
| `dia_semana_nombre` | string | Nombre del d√≠a |
| `semana_anio` | int | Semana del a√±o (1-53) |
| `mes_nombre` | string | Nombre del mes |
| `es_fin_semana` | int | 1 si es s√°bado/domingo, 0 si no |
| `es_inicio_mes` | int | 1 si es d√≠a 1-7, 0 si no |
| `es_fin_mes` | int | 1 si es d√≠a 25+, 0 si no |
| `mes_sin` | float | sin(2œÄ*mes/12) - estacionalidad c√≠clica |
| `mes_cos` | float | cos(2œÄ*mes/12) - estacionalidad c√≠clica |
| `dia_semana_sin` | float | sin(2œÄ*dia_semana/7) |
| `dia_semana_cos` | float | cos(2œÄ*dia_semana/7) |
| `dias_desde_origen` | int | D√≠as desde la primera operaci√≥n |

### **Features Agregadas**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `operacion_id` | int | ID √∫nico secuencial |
| `marca_categoria` | string | Marca (Top 10) u "OTROS" |
| `tipo_categoria` | string | Tipo de veh√≠culo (Top 10) u "OTROS" |

---

## üîß Configuraci√≥n Avanzada

### **Limitar Registros para Testing**

Si quer√©s probar con un subconjunto de datos (m√°s r√°pido):

Edit√° `02_unir_datasets.py`, l√≠nea ~249:
```python
LIMIT = 10000  # Procesar solo 10,000 registros por tabla
```

### **Cambiar Formato de Salida**

Por defecto se guarda en **Parquet**. Si prefer√≠s CSV:

```python
# En lugar de:
df.to_parquet(filepath, index=False)

# Usar:
df.to_csv(filepath.replace('.parquet', '.csv'), index=False)
```

---

## üìà Tama√±o Esperado del Dataset

**Estimaciones (basadas en tablas actuales):**

| Tabla | Registros | Tama√±o en Parquet |
|-------|-----------|-------------------|
| Inscripciones | ~3M | ~300 MB |
| Transferencias | ~9M | ~900 MB |
| Prendas | ~5M | ~500 MB |
| **TOTAL UNIFICADO** | **~17M** | **~1.5 GB** |

*Nota: Parquet con compresi√≥n snappy reduce tama√±o ~60% vs CSV*

---

## ‚úÖ Validaci√≥n del Dataset

El script `02_unir_datasets.py` incluye validaci√≥n autom√°tica:

- ‚úì Total de registros por tipo de operaci√≥n
- ‚úì Rango temporal (fecha min/max)
- ‚úì Distribuci√≥n por provincia
- ‚úì Distribuci√≥n por marca
- ‚úì Valores nulos por columna
- ‚úì Registros por a√±o

---

## üêõ Troubleshooting

### Error: "Connection refused" (PostgreSQL)

**Problema:** PostgreSQL no est√° corriendo.

**Soluci√≥n:**
```bash
# Windows (PowerShell como administrador)
net start postgresql-x64-15

# Verificar que est√© corriendo
psql -U postgres -d mercado_automotor -c "SELECT 1"
```

### Error: "ModuleNotFoundError"

**Problema:** Falta instalar librer√≠as.

**Soluci√≥n:**
```bash
pip install sqlalchemy pandas numpy pyarrow psycopg2-binary
```

### Error: "Table does not exist"

**Problema:** Las tablas no est√°n cargadas en PostgreSQL.

**Soluci√≥n:**
```bash
# Cargar datos primero
python cargar_datos_gob_ar_postgresql.py
```

---

## üìä Pr√≥ximos Pasos (Fase 2)

Una vez que tengas el dataset unificado:

1. ‚úÖ **Crear clientes de APIs** (BCRA, INDEC, CEM)
2. ‚úÖ **Descargar datos macroecon√≥micos**
3. ‚úÖ **Crear dataset macro** (IPC, TC, BADLAR, EMAE, etc.)
4. ‚úÖ **Combinar datasets** (transaccional + macro) por fecha
5. ‚úÖ **Feature engineering avanzado**
6. ‚úÖ **Entrenar modelos de forecasting**

---

## üìù Notas Importantes

- **Backup:** El script NO modifica las tablas originales en PostgreSQL
- **Idempotencia:** Pod√©s ejecutar los scripts m√∫ltiples veces
- **Memoria:** Procesamiento de ~17M registros requiere ~8-16 GB RAM
- **Tiempo:** Ejecuci√≥n completa: ~10-20 minutos (depende del hardware)

---

## üÜò Ayuda

Si ten√©s problemas, verific√°:

1. PostgreSQL est√° corriendo (`psql -U postgres -l`)
2. Las tablas tienen datos (`SELECT COUNT(*) FROM datos_gob_inscripciones`)
3. Ten√©s suficiente espacio en disco (~2 GB libres)
4. Las librer√≠as est√°n instaladas (`pip list | grep pandas`)

---

## üìß Contacto

Para bugs o mejoras, reportar en el repositorio del proyecto.

---

**Fecha:** 2025-11-12
**Versi√≥n:** 1.0
**Autor:** Claude + Usuario
