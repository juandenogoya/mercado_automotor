# üï∑Ô∏è Web Scraping de MercadoLibre

## üìã Resumen

Dado que MercadoLibre cerr√≥ el acceso a su API p√∫blica en 2025, implementamos un **web scraper robusto** para obtener datos del mercado automotor retail.

---

## ‚úÖ Caracter√≠sticas del Scraper

### üõ°Ô∏è Anti-Detecci√≥n
- ‚úÖ **Rotating User-Agents:** Rota entre 6 user agents reales
- ‚úÖ **Rate Limiting:** Delays aleatorios de 3-7 segundos entre requests
- ‚úÖ **Headers realistas:** Imita navegadores reales (Chrome, Firefox, Safari)
- ‚úÖ **Random timing:** Variaci√≥n aleatoria en delays

### üîç Detecci√≥n de Bloqueos
El scraper detecta autom√°ticamente:
- ‚úÖ **Status 403:** IP o User-Agent bloqueado
- ‚úÖ **Status 429:** Rate limit excedido
- ‚úÖ **Status 503:** Servicio temporalmente no disponible
- ‚úÖ **CAPTCHAs:** Detecta p√°ginas de verificaci√≥n
- ‚úÖ **Firewalls:** Cloudflare y otros
- ‚úÖ **P√°ginas de error de ML**
- ‚úÖ **HTML anormalmente corto:** Bloqueo silencioso

### üìä Datos Extra√≠dos
- T√≠tulo completo del veh√≠culo
- Precio (convertido a n√∫mero)
- Marca y modelo (inferidos del t√≠tulo)
- A√±o
- Condici√≥n (0km/usado)
- Ubicaci√≥n (ciudad, provincia)
- URL e ID del listing
- Fecha del snapshot

### üíæ Integraci√≥n con PostgreSQL
- Guarda autom√°ticamente en la tabla `mercadolibre_listings`
- Evita duplicados (mismo item + misma fecha)
- Metadata completa con timestamps

---

## üöÄ Uso B√°sico

### 1. Traer el c√≥digo

```powershell
git pull origin claude/review-project-advantages-011CUvWjZ32MibKBCTEhtWn8
```

### 2. Probar el scraper

```powershell
python test_mercadolibre_scraper.py
```

Este test:
- ‚úÖ Scrapea Toyota (1 p√°gina)
- ‚úÖ Guarda en PostgreSQL
- ‚úÖ Scrapea veh√≠culos 0km
- ‚úÖ Muestra estad√≠sticas de precios
- ‚úÖ Detecta y reporta bloqueos

### 3. Uso program√°tico

```python
from backend.scrapers import MercadoLibreScraper

# Crear scraper
scraper = MercadoLibreScraper()

# Buscar veh√≠culos
result = scraper.search_vehicles(
    marca="Toyota",
    modelo="Hilux",
    condition="new",  # 'new' o 'used'
    max_pages=3
)

# Ver resultados
print(f"Total scrapeado: {result['total_scraped']}")
for item in result['items']:
    print(f"{item['title']} - ${item['price']:,.0f}")

# Guardar en BD
saved = scraper.save_to_database(result['items'])
print(f"Guardados: {saved} items")
```

### 4. Scrapear m√∫ltiples marcas

```python
result = scraper.scrape_and_save(
    marcas=['Toyota', 'Ford', 'Volkswagen'],
    max_items_per_marca=100
)

print(f"Total guardado: {result['total_saved']}")
```

---

## üîç Logging y Diagn√≥stico

### Niveles de Logging

**INFO:** Operaciones normales
```
[MercadoLibre Scraper] Inicializado
[B√∫squeda] Iniciando scraping: marca=Toyota
[P√°gina 1] Scrapeando: https://...
[P√°gina 1] Status 200 - OK
[Resultado] Total items scrapeados: 47
```

**SUCCESS:** Operaciones exitosas
```
[P√°gina 1] Status 200 - OK
[BD] Guardados 45 items nuevos
```

**WARNING:** Advertencias no cr√≠ticas
```
[P√°gina 3] No se encontraron items
[BLOQUEO POSIBLE] HTML muy corto, posible bloqueo silencioso
[BD] Item MLA123 ya existe para hoy
```

**ERROR:** Errores y bloqueos
```
[BLOQUEO] Status 403 Forbidden - IP o User-Agent bloqueado
[BLOQUEO] Status 429 Too Many Requests - Rate limit excedido
[BLOQUEO] CAPTCHA detectado en la p√°gina
[BLOQUEO] Cloudflare/Firewall detectado
```

### Ver Logs

```powershell
# Ver logs en tiempo real
tail -f logs/app.log

# Ver solo errores
grep ERROR logs/app.log

# Ver bloqueos
grep BLOQUEO logs/app.log
```

---

## üö® ¬øQu√© Hacer Si Te Bloquean?

### Identificar el Bloqueo

Buscar en logs:
```powershell
grep BLOQUEO logs/app.log
```

### Tipos de Bloqueo y Soluciones

#### 1. **Status 403 - IP Bloqueada**

**S√≠ntoma:**
```
[BLOQUEO] Status 403 Forbidden - IP o User-Agent bloqueado
```

**Soluciones:**
1. **Esperar:** 15-30 minutos antes de reintentar
2. **Cambiar IP:**
   - Reiniciar router
   - Usar VPN
   - Usar proxy
3. **Aumentar delays:**
   Editar `.env`:
   ```env
   SCRAPING_DELAY_MIN=5
   SCRAPING_DELAY_MAX=10
   ```

#### 2. **Status 429 - Rate Limit**

**S√≠ntoma:**
```
[BLOQUEO] Status 429 Too Many Requests - Rate limit excedido
```

**Soluciones:**
1. **Reducir velocidad:**
   ```env
   SCRAPING_DELAY_MIN=7
   SCRAPING_DELAY_MAX=15
   ```
2. **Scrapear menos p√°ginas:** `max_pages=2`
3. **Esperar 1 hora** antes de reintentar

#### 3. **CAPTCHA**

**S√≠ntoma:**
```
[BLOQUEO] CAPTCHA detectado en la p√°gina
```

**Soluciones:**
1. **Resolver manualmente:** Abrir ML en navegador, resolver CAPTCHA
2. **Cambiar IP**
3. **Esperar 30-60 minutos**
4. **Usar servicio de CAPTCHA solving** (avanzado, no recomendado)

#### 4. **Cloudflare/Firewall**

**S√≠ntoma:**
```
[BLOQUEO] Cloudflare/Firewall detectado
```

**Soluciones:**
1. **Cambiar User-Agent:**
   El scraper rota autom√°ticamente, pero si persiste, actualizar la lista en el c√≥digo
2. **Usar Selenium** en lugar de requests (m√°s lento pero m√°s realista)
3. **Cambiar IP/VPN**

---

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Ajustar Delays

En `.env`:
```env
# Delay m√≠nimo entre requests (segundos)
SCRAPING_DELAY_MIN=3

# Delay m√°ximo entre requests (segundos)
SCRAPING_DELAY_MAX=7

# Timeout de requests (segundos)
SCRAPING_TIMEOUT=30
```

**Recomendaciones:**
- **Conservador:** MIN=5, MAX=10 (m√°s lento, m√°s seguro)
- **Normal:** MIN=3, MAX=7 (balance)
- **Agresivo:** MIN=2, MAX=5 (m√°s r√°pido, mayor riesgo)

### Usar Proxies (Avanzado)

```python
scraper = MercadoLibreScraper()

# Configurar proxy
scraper.session.proxies = {
    'http': 'http://proxy.ejemplo.com:8080',
    'https': 'https://proxy.ejemplo.com:8080'
}

# O con autenticaci√≥n
scraper.session.proxies = {
    'http': 'http://usuario:pass@proxy.com:8080',
    'https': 'https://usuario:pass@proxy.com:8080'
}
```

### Rotating Proxies

```python
import random

proxies_list = [
    'http://proxy1.com:8080',
    'http://proxy2.com:8080',
    'http://proxy3.com:8080',
]

# Antes de cada request
scraper.session.proxies = {
    'http': random.choice(proxies_list),
    'https': random.choice(proxies_list)
}
```

---

## üìä Automatizaci√≥n

### Scraping Diario con Cron (Linux/Mac)

```bash
# Editar crontab
crontab -e

# Agregar l√≠nea (scrapear a las 3 AM diariamente)
0 3 * * * cd /path/to/mercado_automotor && python scripts/scrape_mercadolibre.py
```

### Scraping Diario con Task Scheduler (Windows)

1. Crear script `scripts/scrape_mercadolibre.bat`:
   ```batch
   @echo off
   cd C:\path\to\mercado_automotor
   python scripts/scrape_mercadolibre.py
   ```

2. Abrir "Programador de tareas" (Task Scheduler)
3. Crear tarea b√°sica
4. Trigger: Diario, 3:00 AM
5. Acci√≥n: Ejecutar `scrape_mercadolibre.bat`

### Con Airflow (Recomendado)

Crear DAG en `airflow/dags/mercadolibre_daily_scrape.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def scrape_mercadolibre():
    from backend.scrapers import MercadoLibreScraper
    scraper = MercadoLibreScraper()
    result = scraper.scrape_and_save(
        marcas=['Toyota', 'Ford', 'Volkswagen', 'Chevrolet'],
        max_items_per_marca=200
    )
    return result

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(hours=1),
}

dag = DAG(
    'mercadolibre_daily_scrape',
    default_args=default_args,
    schedule_interval='0 3 * * *',  # 3 AM diario
    catchup=False
)

scrape_task = PythonOperator(
    task_id='scrape_ml',
    python_callable=scrape_mercadolibre,
    dag=dag
)
```

---

## üìà An√°lisis de Datos Scrapeados

### Query SQL - Precios Promedio por Marca

```sql
SELECT
    marca,
    condicion,
    COUNT(*) as cantidad,
    AVG(precio) as precio_promedio,
    MIN(precio) as precio_min,
    MAX(precio) as precio_max
FROM mercadolibre_listings
WHERE fecha_snapshot = CURRENT_DATE
GROUP BY marca, condicion
ORDER BY cantidad DESC;
```

### Query SQL - Evoluci√≥n de Precios

```sql
SELECT
    fecha_snapshot,
    marca,
    AVG(precio) as precio_promedio
FROM mercadolibre_listings
WHERE marca = 'Toyota'
  AND modelo LIKE '%Hilux%'
  AND condicion = 'new'
GROUP BY fecha_snapshot, marca
ORDER BY fecha_snapshot;
```

### Python - An√°lisis de Oferta

```python
from backend.utils.database import get_db
from backend.models.mercadolibre_listings import MercadoLibreListing
from sqlalchemy import func
from datetime import date

with get_db() as db:
    # Contar oferta por marca
    oferta = db.query(
        MercadoLibreListing.marca,
        func.count(MercadoLibreListing.id).label('cantidad')
    ).filter(
        MercadoLibreListing.fecha_snapshot == date.today()
    ).group_by(
        MercadoLibreListing.marca
    ).order_by(
        func.count(MercadoLibreListing.id).desc()
    ).limit(10).all()

    for marca, cantidad in oferta:
        print(f"{marca}: {cantidad} listings")
```

---

## üéØ Mejores Pr√°cticas

### ‚úÖ DO
- ‚úÖ Usar delays generosos (3-7 segundos)
- ‚úÖ Scrapear durante horas de bajo tr√°fico (3-6 AM)
- ‚úÖ Monitorear logs constantemente
- ‚úÖ Guardar datos inmediatamente en BD
- ‚úÖ Implementar retry logic con backoff exponencial
- ‚úÖ Respetar rate limits

### ‚ùå DON'T
- ‚ùå Scrapear sin delays
- ‚ùå Hacer requests paralelas
- ‚ùå Ignorar mensajes de bloqueo
- ‚ùå Scrapear durante horas pico
- ‚ùå Re-scrapear mismos datos m√∫ltiples veces al d√≠a
- ‚ùå Ignorar errores 429/503

---

## üîß Troubleshooting

### Problema: "No se encontraron items"

**Posibles causas:**
1. MercadoLibre cambi√≥ estructura HTML
2. B√∫squeda sin resultados
3. Bloqueo silencioso

**Soluci√≥n:**
```python
# Verificar que la URL sea correcta
result = scraper.search_vehicles(marca="Toyota", max_pages=1)
if result['items']:
    print(result['items'][0])  # Ver estructura
```

### Problema: "HTML muy corto"

**Causa:** Bloqueo silencioso o p√°gina vac√≠a

**Soluci√≥n:**
1. Verificar URL manualmente en navegador
2. Esperar 30 minutos
3. Cambiar IP

### Problema: Items sin precio

**Causa:** Estructura HTML diferente

**Soluci√≥n:**
- Actualizar selectores en `_parse_item()`
- Verificar HTML manualmente
- Reportar para actualizar scraper

---

## üìû Soporte

Si encontr√°s problemas:
1. Revisar logs: `logs/app.log`
2. Buscar mensajes de BLOQUEO
3. Verificar configuraci√≥n en `.env`
4. Probar con delays m√°s altos

---

## ‚öñÔ∏è Consideraciones Legales

‚ö†Ô∏è **IMPORTANTE:** Web scraping puede estar sujeto a t√©rminos de servicio.

- Us√° el scraper de forma √©tica y responsable
- Respet√° rate limits
- No sobrecargues los servidores de ML
- Los datos son solo para an√°lisis privado
- No redistribuir datos scrapeados sin autorizaci√≥n

---

## üöÄ Pr√≥ximos Pasos

Una vez que el scraper funcione:

1. **Automatizar scraping diario**
2. **Crear dashboard Streamlit** con datos scraped
3. **Combinar con datos.gob.ar** para an√°lisis completo
4. **Alertas autom√°ticas** de oportunidades
5. **An√°lisis de tendencias** de precios

¬°√âxito con el scraping! üï∑Ô∏è
